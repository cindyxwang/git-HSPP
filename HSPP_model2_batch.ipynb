{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import os\n",
    "from petl import fromcsv, look, cut, tocsv \n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import vincenty\n",
    "from scipy.stats import norm\n",
    "from numpy import linspace\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from pylab import plot,show,hist,figure,title\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load and merge the tables scrapted from public datatbase\n",
    "\n",
    "def p2f(x):\n",
    "    return float(x.strip('%'))/100\n",
    "\n",
    "county2labor = pd.read_csv('CA_county_employment.csv').as_matrix()\n",
    "for i in range(1,county2labor.shape[0]):\n",
    "    county2labor[i,5] = p2f(county2labor[i,5])\n",
    "county2LF=county2labor[:,[0,2,5]]\n",
    "for i in range(1,county2LF.shape[0]):\n",
    "    county2LF[i,1]=int(county2LF[i,1])\n",
    "county2LF[0,2] ='unemploy_rate'\n",
    "\n",
    "propTax = pd.read_csv('CA_propTax.csv').as_matrix()\n",
    "propTax[22,7] = '0'\n",
    "propTax[58,7] = '0'\n",
    "\n",
    "for i in range(1,propTax.shape[0]):\n",
    "    propTax[i,7] = p2f(propTax[i,7])\n",
    "propTax[22,7] = np.median(propTax[:,7])\n",
    "propTax[58,7] = np.median(propTax[:,7])\n",
    "\n",
    "propTax[0,0]='County'\n",
    "propTax = propTax[:,[0,7]]\n",
    "propTax[0,1]='midTax'\n",
    "\n",
    "zip2county = pd.read_csv('CA_zip2county.csv').as_matrix()\n",
    "zip2county = zip2county[:,[0,2]]\n",
    "zip2county[0,0]='zipcd'\n",
    "zip2county[0,1]='County'\n",
    "\n",
    "for i in range(1,zip2county.shape[0]):\n",
    "    zip2county[i,0]=int(zip2county[i,0])\n",
    "\n",
    "\n",
    "df1=pd.DataFrame(county2LF, columns=['County', 'Labor Force', 'unemploy_rate'])\n",
    "df2=pd.DataFrame(propTax, columns=['County', 'midTax'])\n",
    "df3=pd.DataFrame(zip2county, columns=['zipcd', 'County'])\n",
    "\n",
    "['zipcd', 'County','Labor Force', 'unemploy_rate', 'midTax']\n",
    "\n",
    "df_merge=df1.merge(df2,on='County')\n",
    "df_merge_merge=df3.merge(df_merge,on='County')\n",
    "df_zip2countyInfo = df_merge_merge.ix[1:]\n",
    "#df_zip2countyInfo =['County', 'Labor Force', 'unemploy_rate','midTax','zipcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cindy/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning:\n",
      "\n",
      "Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the previous transaction data \n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole\n",
    "data = pd.read_csv('soldPend_home_all.csv').as_matrix()\n",
    "\n",
    "low_memory=False\n",
    "mylist=[]\n",
    "u, indices =np.unique(data[:,3], return_index=True)\n",
    "data_uni = data[indices,:]\n",
    "data=data_uni\n",
    "                            \n",
    "\n",
    "idx=[0,1,5,7,18]\n",
    "data[:,idx]=data[:,idx]/1000 #convert price unit to $k\n",
    "data[:,11]=2016-data[:,11] #convert year_built to age\n",
    "\n",
    "idx_forFlip=np.where(data[:,16]==True)\n",
    "data[:,16]=1\n",
    "data[idx_forFlip,16]=2\n",
    "\n",
    "idx_takedown=np.where(data[:,6]==True)\n",
    "data[:,6]=1\n",
    "data[idx_takedown,6]=2\n",
    "\n",
    "\n",
    "#Handeling the non-exist and mis-input zipcode. Currently just not using those data. \n",
    "#It can be modified in the future to get zipcode from latitude and longitude\n",
    "# if those are available.\n",
    "\n",
    "noZip = pd.isnull(data[:,2])\n",
    "idx_noZip=np.where(noZip==True)\n",
    "data = np.delete(data,idx_noZip,0)\n",
    "\n",
    "for i in range(0,data.shape[0]):\n",
    "    if (type(data[i,2]) is str):\n",
    "        #print data[i,2]\n",
    "        data[i,2] = int(data[i,2][:5])\n",
    "        #print data[i,2]\n",
    "idx_misZip=np.where(data[:,2]<90000)   \n",
    "data = np.delete(data,idx_misZip,0)\n",
    "\n",
    "\n",
    "idx_soldyearFilter=np.where(data[:,13]>10000)\n",
    "data=np.delete(data,idx_soldyearFilter,0)\n",
    "idx_lotFilter=np.where(data[:,12]>2*pow(10,11))\n",
    "data=np.delete(data,idx_lotFilter,0)\n",
    "idx_sqftFilter=np.where(data[:,17]>400000)\n",
    "data=np.delete(data,idx_sqftFilter,0)\n",
    "idx_misSqft=np.where(data[:,17]<20)\n",
    "data=np.delete(data,idx_misSqft,0)\n",
    "                 \n",
    "idx_priceFilter=np.where(data[:,18]>200000)\n",
    "data=np.delete(data,idx_priceFilter,0)\n",
    "\n",
    "#handling the unreasonable and irrelevant year_built\n",
    "idx_yearFilter=np.where(data[:,11]<0)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "idx_yearFilter=np.where(data[:,11]>500)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "\n",
    "data_forComp=data\n",
    "\n",
    "col_null = pd.isnull(data[:,1])\n",
    "idx_prepriceExist=np.where(col_null==True)\n",
    "data=np.delete(data,idx_prepriceExist,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imputing #bed and #bath\n",
    "numBath_mis=np.where(data[:,4]>100) #remove an outlier in the numBath\n",
    "data=np.delete(data,numBath_mis,0)\n",
    "\n",
    "noBath = pd.isnull(data[:,4])\n",
    "idx_noBath=np.where(noBath==True)\n",
    "data[idx_noBath,4]=np.median(data[:,4])\n",
    "\n",
    "idx_0Bath = np.where(data[:,4]==0)\n",
    "data[idx_0Bath,4]=np.median(data[:,4])\n",
    "\n",
    "noBed = pd.isnull(data[:,10])\n",
    "idx_noBed=np.where(noBed==True)\n",
    "data[idx_noBed,9]=np.median(data[:,10])\n",
    "\n",
    "idx_0Bed = np.where(data[:,10]==0)\n",
    "data[idx_0Bed,9]=np.median(data[:,10])\n",
    "\n",
    "#drop the rows where there are still features missing\n",
    "for i in range(0,19):\n",
    "    col_null = pd.isnull((data[:,i]))\n",
    "    idx=np.where(col_null==True)\n",
    "    data=np.delete(data,idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the previous transaction data with transaction data\n",
    "\n",
    "df=pd.DataFrame(data, columns=['pre_sqft_price','pre_price','zipcd','propID','numBath','sqft_price','takedown',\\\n",
    "                               'cur_lis_price','lat','long','numBed','year_built','lot_sqft','numDays','year',\\\n",
    "                               'month','forFlip','sqft','price'])\n",
    "\n",
    "df_statbyZip = df['price'].astype(float).groupby(df['zipcd']).agg([np.median,np.mean,np.std,np.max,np.min,len])\n",
    "\n",
    "data_statbyZip = df_statbyZip.as_matrix()\n",
    "b=(np.unique(data[:,2])).reshape((df_statbyZip.shape[0],1))\n",
    "data_statbyZip=np.append(data_statbyZip,b,1)\n",
    "\n",
    "df_statbyZip=pd.DataFrame(data_statbyZip, columns=['median','mean','std','max','min','count','zipcd'])\n",
    "df_additional = df_statbyZip.merge(df_zip2countyInfo,on='zipcd')\n",
    "df_all=df.merge(df_additional,on='zipcd')\n",
    "\n",
    "data_all=df_all.as_matrix()\n",
    "data_all[:,[18,28]]=data_all[:,[28,18]]\n",
    "data_all[:,[0,3]]=data_all[:,[3,0]]\n",
    "data_all[:,[1,2]]=data_all[:,[2,1]]\n",
    "data_all[:,[8,9]]=data_all[:,[9,8]]\n",
    "#remove houses that has most likely mis input of sqft and which that has unreasonalbe price per sqft\n",
    "\n",
    "var_noNeed=[3,5,7,20,21,22,23,25]\n",
    "data_model=np.delete(data_all,var_noNeed,1)  #delete the variable that are not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_all\n",
    "fields_data_all=['propID','zipcd','pre_price','pre_sqft_price','numBath','sqft_price','takedown','cur_lis_price',\\\n",
    "             'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','mean','std','max','min','len','County','Labor Force',\\\n",
    "        'unemploy_rate','price']\n",
    "for index in np.arange(len(fields_data_all)):\n",
    "    print str(index) + \",\" + fields_data_all[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_model\n",
    "fields_data_model=['propID','zipcd','pre_price','numBath','takedown',\\\n",
    "             'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "for index in np.arange(len(fields_data_model)):\n",
    "    print str(index) + \",\" + fields_data_model[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pctErr_top10comp(testID):\n",
    "    \n",
    "    Y_train_compa = np.delete(data_model[:,20],testID,0)\n",
    "    \n",
    "    feature1_compa=np.delete(data_model[:,:20],testID,0)\n",
    "    loc_test = data_model[testID,:20]\n",
    "    Y_test = data_model[testID,20]\n",
    "    idx_numDaysFilter_compa = np.where(feature1_compa[:,10]>(loc_test[10]+180))\n",
    "    idx_numDaysFilter_compa = np.asarray(idx_numDaysFilter_compa)\n",
    "    feature1_compa=np.delete(feature1_compa,idx_numDaysFilter_compa,0)\n",
    "    Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter_compa,0)\n",
    "\n",
    "    idx_numDaysFilter2_compa = np.where(feature1_compa[:,10] < loc_test[10])\n",
    "    idx_numDaysFilter2_compa = np.asarray(idx_numDaysFilter2_compa)\n",
    "    feature1_compa=np.delete(feature1_compa,idx_numDaysFilter2_compa,0)\n",
    "    Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter2_compa,0)   \n",
    "    \n",
    "    idx_flipFilter_compa = np.where(feature1_compa[:,13] != loc_test[13])\n",
    "    idx_flipFilter_compa = np.asarray(idx_flipFilter_compa)\n",
    "    feature1_compa=np.delete(feature1_compa,idx_flipFilter_compa,0)\n",
    "    Y_train_compa=np.delete(Y_train_compa,idx_flipFilter_compa,0)\n",
    "\n",
    "    idx_tdFilter_compa = np.where(feature1_compa[:,4] != loc_test[4])\n",
    "    idx_tdFilter_compa = np.asarray(idx_tdFilter_compa)\n",
    "    feature1_compa=np.delete(feature1_compa,idx_tdFilter_compa,0)\n",
    "    Y_train_compa=np.delete(Y_train_compa,idx_tdFilter_compa,0)    \n",
    "    feature_test_compa =np.append(loc_test, 0)    \n",
    "    Y_train_compa_backup = Y_train_compa\n",
    "    feature1_compa_backup=feature1_compa\n",
    "    \n",
    "    def price_sqft_filter(price_match,sqft_match,Y_train_compa_backup,feature1_compa_backup,loc_test):\n",
    "        idx_priceFilter_compa = np.where(np.absolute(np.divide(Y_train_compa_backup-loc_test[2],loc_test[2]))>price_match)\n",
    "        idx_priceFilter_compa = np.asarray(idx_priceFilter_compa)\n",
    "        feature1_compa=np.delete(feature1_compa_backup,idx_priceFilter_compa,0)\n",
    "        Y_train_compa=np.delete(Y_train_compa_backup,idx_priceFilter_compa,0)\n",
    "\n",
    "        idx_sqftFilter_compa = np.where(np.abs(np.divide(feature1_compa[:,14]-loc_test[14],loc_test[14]))>sqft_match)\n",
    "        idx_sqftFilter_compa = np.asarray(idx_sqftFilter_compa)\n",
    "        feature1_compa=np.delete(feature1_compa,idx_sqftFilter_compa,0)\n",
    "        Y_train_compa=np.delete(Y_train_compa,idx_sqftFilter_compa,0)\n",
    "        return (feature1_compa, Y_train_compa)\n",
    "    \n",
    "    price_match = 0.1\n",
    "    sqft_match = 0.2\n",
    "    feature1_compa, Y_train_compa = price_sqft_filter(price_match,sqft_match,Y_train_compa_backup,feature1_compa_backup,loc_test)\n",
    "    \n",
    "    top_compa= 30   \n",
    "    if feature1_compa.shape[0]<top_compa:\n",
    "        if feature1_compa.shape[0]>10:\n",
    "            top_compa = feature1_compa.shape[0]\n",
    "        else:\n",
    "            feature1_compa, Y_train_compa = price_sqft_filter(price_match+0.05,sqft_match+0.1,Y_train_compa_backup,feature1_compa_backup,loc_test)\n",
    "            if feature1_compa.shape[0]<10:                \n",
    "                feature1_compa, Y_train_compa = price_sqft_filter(price_match+0.1,sqft_match+0.2,Y_train_compa_backup,feature1_compa_backup,loc_test)              \n",
    "                if feature1_compa.shape[0]<10:\n",
    "                    feature1_compa, Y_train_compa = price_sqft_filter(price_match+0.3,sqft_match+0.5,Y_train_compa_backup,feature1_compa_backup,loc_test)              \n",
    "                    if feature1_compa.shape[0]==0:\n",
    "                        feature1_compa = loc_test.reshape((1,loc_test.shape[0]))\n",
    "                        Y_train_compa = np.asarray([1]) \n",
    "            if feature1_compa.shape[0]<top_compa:\n",
    "                top_compa = feature1_compa.shape[0]\n",
    "    dist_compa=[]\n",
    "    for i in range(0,feature1_compa.shape[0]):\n",
    "            dist_compa.append(vincenty(loc_test[5:7], feature1_compa[i,5:7]).miles)\n",
    " \n",
    "    dist_compa=np.asarray(dist_compa)\n",
    "    dist_compa_sorted = np.sort(dist_compa)\n",
    "        \n",
    "    dist_select_compa= dist_compa_sorted[0:top_compa]\n",
    "    loc_match_id_compa=[np.where(dist_compa == dist_select_compa[x])[0] for x in range(0,top_compa)]\n",
    "    loc_match_id_compa = np.concatenate(loc_match_id_compa).ravel()\n",
    "    loc_match_id_compa = np.unique(loc_match_id_compa)\n",
    "    dist_selected_compa = dist_compa_sorted[0:top_compa]\n",
    "    dist_feature_compa = dist_compa[loc_match_id_compa]\n",
    "    Y_train_compa = Y_train_compa[loc_match_id_compa]\n",
    "\n",
    "\n",
    "    feature1_compa = feature1_compa[loc_match_id_compa,:]\n",
    "    feature_compa=np.vstack((feature1_compa.T, dist_feature_compa.T))\n",
    "    feature_compa=feature_compa.T\n",
    "\n",
    "    feature_compa = np.vstack(feature_compa[:, 3:]).astype(np.float)\n",
    "    feature_test_compa = np.vstack(feature_test_compa[3:]).astype(np.float)\n",
    "\n",
    "    scaler_compa = preprocessing.StandardScaler().fit(feature_compa)\n",
    "    X_scaled_compa = scaler_compa.transform(feature_compa)  \n",
    "    X_scaled_normed_compa = preprocessing.normalize(X_scaled_compa, norm='l2')\n",
    "\n",
    "    X_train_compa = X_scaled_normed_compa\n",
    "\n",
    "    X_test_scaled_compa = scaler_compa.transform(feature_test_compa.reshape(1, -1))  \n",
    "    X_test_scaled_normed_compa= preprocessing.normalize(X_test_scaled_compa.reshape(1, -1) , norm='l2')\n",
    "    X_test_compa = X_test_scaled_normed_compa\n",
    "\n",
    "    print str(X_train_compa.shape[0]) + (' houses are used for comparable house search')\n",
    "\n",
    "    import math\n",
    "    from itertools import izip\n",
    "\n",
    "    def dot_product(v1, v2):\n",
    "        return sum(map(lambda x: x[0] * x[1], izip(v1, v2)))\n",
    "\n",
    "    def cosine_measure(v1, v2):\n",
    "        prod = dot_product(v1, v2)\n",
    "        len1 = math.sqrt(dot_product(v1, v1))\n",
    "        len2 = math.sqrt(dot_product(v2, v2))\n",
    "        return prod / (len1 * len2)\n",
    "\n",
    "    similarity = []\n",
    "    for i in range(0,X_train_compa.shape[0]):\n",
    "        similarity.append(cosine_measure(X_test_compa.reshape((-1,1)), X_train_compa[i,:].reshape((-1,1))) )\n",
    "\n",
    "\n",
    "\n",
    "    top_simi = 10\n",
    "    if len(similarity)<30:\n",
    "        top_simi = len(similarity)\n",
    "    simi_sorted = sorted(similarity, reverse=True)\n",
    "    simi_select = simi_sorted[0:top_simi]\n",
    "    simi_match_id =[np.where(similarity == simi_select[x])[0] for x in range(0,top_simi)]\n",
    "    simi_match_id = np.concatenate(simi_match_id).ravel()\n",
    "    simi_match_id = np.unique(simi_match_id)  \n",
    "    sold_price = Y_train_compa[simi_match_id]\n",
    "    simi = np.asarray(similarity)[simi_match_id]\n",
    "    simi=simi.reshape((-1,1))   \n",
    "    weight = np.asarray(simi/np.sum(simi)).reshape((simi.shape[0],))\n",
    "    price_est_weighted=np.sum(np.multiply(sold_price,weight))\n",
    "    price_est_median=np.median(sold_price)\n",
    "    err_median = price_est_median-Y_test\n",
    "    err_weighted = price_est_weighted-Y_test\n",
    "    pctErr_median = np.divide(err_median,Y_test)\n",
    "    pctErr_weighted = np.divide(err_weighted,Y_test)\n",
    "    pctErr_median=pctErr_median\n",
    "    return (pctErr_median, pctErr_weighted,Y_test)   \n",
    "\n",
    "pctErrM = []\n",
    "pctErrW = []\n",
    "Y=[]\n",
    "num_test=0\n",
    "group=0\n",
    "for i in range(0,280000,1000):\n",
    "    pctErr_median, pctErr_weighted,Y_test = pctErr_top10comp(i)\n",
    "    pctErrM.append(pctErr_median)\n",
    "    pctErrW.append(pctErr_weighted)\n",
    "    Y.append(Y_test)\n",
    "    num_test=num_test+1\n",
    "    \n",
    "    filename='pctErr'+str(group)\n",
    "    if num_test == 20:\n",
    "        scipy.io.savemat('pctErr'+str(group)+'_top5000.mat', {'pctErrMedian':pctErrM,'pctErrWeighted':pctErrW,'Y_test':Y})\n",
    "        group=group+1\n",
    "        num_test=num_test-20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
