{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import os\n",
    "from petl import fromcsv, look, cut, tocsv \n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import vincenty\n",
    "from scipy.stats import norm\n",
    "from numpy import linspace\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from pylab import plot,show,hist,figure,title\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing the data scrapted from public database\n",
    "\n",
    "def p2f(x):\n",
    "    return float(x.strip('%'))/100\n",
    "\n",
    "county2labor = pd.read_csv('CA_county_employment.csv').as_matrix()\n",
    "for i in range(1,county2labor.shape[0]):\n",
    "    county2labor[i,5] = p2f(county2labor[i,5])\n",
    "county2LF=county2labor[:,[0,2,5]]\n",
    "for i in range(1,county2LF.shape[0]):\n",
    "    county2LF[i,1]=int(county2LF[i,1])\n",
    "county2LF[0,2] ='unemploy_rate'\n",
    "\n",
    "propTax = pd.read_csv('CA_propTax.csv').as_matrix()\n",
    "propTax[22,7] = '0'\n",
    "propTax[58,7] = '0'\n",
    "\n",
    "for i in range(1,propTax.shape[0]):\n",
    "    propTax[i,7] = p2f(propTax[i,7])\n",
    "propTax[22,7] = np.median(propTax[:,7])\n",
    "propTax[58,7] = np.median(propTax[:,7])\n",
    "\n",
    "propTax[0,0]='County'\n",
    "propTax = propTax[:,[0,7]]\n",
    "propTax[0,1]='midTax'\n",
    "\n",
    "zip2county = pd.read_csv('CA_zip2county.csv').as_matrix()\n",
    "zip2county = zip2county[:,[0,2]]\n",
    "zip2county[0,0]='zipcd'\n",
    "zip2county[0,1]='County'\n",
    "\n",
    "for i in range(1,zip2county.shape[0]):\n",
    "    zip2county[i,0]=int(zip2county[i,0])\n",
    "\n",
    "df1=pd.DataFrame(county2LF, columns=['County', 'Labor Force', 'unemploy_rate'])\n",
    "df2=pd.DataFrame(propTax, columns=['County', 'midTax'])\n",
    "df3=pd.DataFrame(zip2county, columns=['zipcd', 'County'])\n",
    "\n",
    "df_merge=df1.merge(df2,on='County')\n",
    "df_merge_merge=df3.merge(df_merge,on='County')\n",
    "df_zip2countyInfo = df_merge_merge.ix[1:] # zipcd',County', 'Labor Force', 'unemploy_rate', 'midTax'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the previous transaction data \n",
    "%matplotlib inline\n",
    "#%qtconsole\n",
    "data = pd.read_csv('soldPend_home_all.csv').as_matrix()\n",
    "\n",
    "low_memory=False\n",
    "mylist=[]\n",
    "u, indices =np.unique(data[:,3], return_index=True)\n",
    "data_uni = data[indices,:]\n",
    "data=data_uni\n",
    "                            \n",
    "\n",
    "idx=[0,1,5,7,18]\n",
    "data[:,idx]=data[:,idx]/1000 #convert price unit to $k\n",
    "data[:,11]=2016-data[:,11] #convert year_built to age\n",
    "\n",
    "idx_forFlip=np.where(data[:,16]==True)\n",
    "data[:,16]=1\n",
    "data[idx_forFlip,16]=2\n",
    "\n",
    "idx_takedown=np.where(data[:,6]==True)\n",
    "data[:,6]=1\n",
    "data[idx_takedown,6]=2\n",
    "\n",
    "\n",
    "#Handeling the non-exist and mis-input zipcode. Currently just not using those data. \n",
    "#It can be modified in the future to get zipcode from latitude and longitude\n",
    "# if those are available.\n",
    "noZip = pd.isnull(data[:,2])\n",
    "idx_noZip=np.where(noZip==True)\n",
    "data = np.delete(data,idx_noZip,0)\n",
    "\n",
    "for i in range(0,data.shape[0]):\n",
    "    if (type(data[i,2]) is str):\n",
    "        #print data[i,2]\n",
    "        data[i,2] = int(data[i,2][:5])\n",
    "        #print data[i,2]\n",
    "idx_misZip=np.where(data[:,2]<90000)   \n",
    "data = np.delete(data,idx_misZip,0)\n",
    "\n",
    "\n",
    "idx_soldyearFilter=np.where(data[:,13]>10000)\n",
    "data=np.delete(data,idx_soldyearFilter,0)\n",
    "idx_lotFilter=np.where(data[:,12]>2*pow(10,11))\n",
    "data=np.delete(data,idx_lotFilter,0)\n",
    "idx_sqftFilter=np.where(data[:,17]>400000)\n",
    "data=np.delete(data,idx_sqftFilter,0)\n",
    "idx_misSqft=np.where(data[:,17]<20)\n",
    "data=np.delete(data,idx_misSqft,0)\n",
    "                 \n",
    "idx_priceFilter=np.where(data[:,18]>200000)\n",
    "data=np.delete(data,idx_priceFilter,0)\n",
    "\n",
    "#handling the unreasonable and irrelevant year_built\n",
    "idx_yearFilter=np.where(data[:,11]<0)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "idx_yearFilter=np.where(data[:,11]>500)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "\n",
    "data_forComp=data\n",
    "\n",
    "col_null = pd.isnull(data[:,1])\n",
    "idx_prepriceExist=np.where(col_null==True)\n",
    "data=np.delete(data,idx_prepriceExist,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imputing #bed and #bath\n",
    "numBath_mis=np.where(data[:,4]>100) #remove an outlier in the numBath\n",
    "data=np.delete(data,numBath_mis,0)\n",
    "\n",
    "noBath = pd.isnull(data[:,4])\n",
    "idx_noBath=np.where(noBath==True)\n",
    "data[idx_noBath,4]=np.median(data[:,4])\n",
    "\n",
    "idx_0Bath = np.where(data[:,4]==0)\n",
    "data[idx_0Bath,4]=np.median(data[:,4])\n",
    "\n",
    "noBed = pd.isnull(data[:,10])\n",
    "idx_noBed=np.where(noBed==True)\n",
    "data[idx_noBed,9]=np.median(data[:,10])\n",
    "\n",
    "idx_0Bed = np.where(data[:,10]==0)\n",
    "data[idx_0Bed,9]=np.median(data[:,10])\n",
    "\n",
    "#drop the rows where there are still features missing\n",
    "for i in range(0,19):\n",
    "    col_null = pd.isnull((data[:,i]))\n",
    "    idx=np.where(col_null==True)\n",
    "    data=np.delete(data,idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge the previous transaction data with scraped public data\n",
    "\n",
    "df=pd.DataFrame(data, columns=['pre_sqft_price','pre_price','zipcd','propID','numBath','sqft_price','takedown',\\\n",
    "                               'cur_lis_price','lat','long','numBed','year_built','lot_sqft','numDays','year',\\\n",
    "                               'month','forFlip','sqft','price'])\n",
    "\n",
    "df_statbyZip = df['price'].astype(float).groupby(df['zipcd']).agg([np.median,np.mean,np.std,np.max,np.min,len])\n",
    "\n",
    "data_statbyZip = df_statbyZip.as_matrix()\n",
    "b=(np.unique(data[:,2])).reshape((df_statbyZip.shape[0],1))\n",
    "data_statbyZip=np.append(data_statbyZip,b,1)\n",
    "\n",
    "df_statbyZip=pd.DataFrame(data_statbyZip, columns=['median','mean','std','max','min','count','zipcd'])\n",
    "df_additional = df_statbyZip.merge(df_zip2countyInfo,on='zipcd')\n",
    "df_all=df.merge(df_additional,on='zipcd')\n",
    "\n",
    "data_all=df_all.as_matrix()\n",
    "data_all[:,[18,28]]=data_all[:,[28,18]]\n",
    "data_all[:,[0,3]]=data_all[:,[3,0]]\n",
    "data_all[:,[1,2]]=data_all[:,[2,1]]\n",
    "data_all[:,[8,9]]=data_all[:,[9,8]]\n",
    "#remove houses that has most likely mis input of sqft and which that has unreasonalbe price per sqft\n",
    "\n",
    "var_noNeed=[3,5,7,20,21,22,23,25]\n",
    "data_model=np.delete(data_all,var_noNeed,1)  #delete the variable that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_all\n",
    "fields_data_all=['propID','zipcd','pre_price','pre_sqft_price','numBath','sqft_price','takedown','cur_lis_price',\\\n",
    "             'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','mean','std','max','min','len','County','Labor Force',\\\n",
    "        'unemploy_rate','price']\n",
    "#for index in np.arange(len(fields_data_all)):\n",
    "    #print str(index) + \",\" + fields_data_all[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_model\n",
    "fields_data_model=['propID','zipcd','pre_price','numBath','takedown',\\\n",
    "             'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "#for index in np.arange(len(fields_data_model)):\n",
    "    #print str(index) + \",\" + fields_data_model[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pctErr_cal(testID):\n",
    "\n",
    "    # generate train and test set\n",
    "    #print ('The house interested is: ') + str(data_model[testID,0])\n",
    "    #print ('It was sold ') + str(data_model[testID,10]) + ' days ago at $' + str(data_model[testID,20]) +'k'\n",
    "\n",
    "    Y_test=data_model[testID,20] -data_model[testID,2]   # for sold price-list price\n",
    "    Y_train = np.delete(data_model[:,20]-data_model[:,2], testID, 0)# for sold price, 20; for list price, 2\n",
    "    loc_train = np.delete(data_model[:,:20], testID, 0)\n",
    "    loc_test = data_model[testID,:20]\n",
    "\n",
    "    dist_miles=[]\n",
    "    for i in range(0,loc_train.shape[0]):\n",
    "        dist_miles.append(vincenty(loc_test[5:7], loc_train[i,5:7]).miles)\n",
    "\n",
    "    dist=np.asarray(dist_miles)\n",
    "    dist_sorted = np.sort(dist)\n",
    "\n",
    "    top = 5000\n",
    "    dist_selected = dist_sorted[0:top]\n",
    "\n",
    "    loc_match_id=[np.where(dist == dist_selected[x])[0] for x in range(0,top)]\n",
    "    loc_match_id = np.concatenate(loc_match_id).ravel()\n",
    "    loc_match_id = np.unique(loc_match_id)\n",
    "\n",
    "    feature1=loc_train[loc_match_id,2:]\n",
    "    dist_feature = dist[loc_match_id]\n",
    "    Y_train = Y_train[loc_match_id]\n",
    "     \n",
    "    feature=np.vstack((feature1.T, dist_feature.T))\n",
    "    feature=feature.T    \n",
    "    \n",
    "    idx_priceFilter = np.where(Y_train[:]>3*loc_test[2])\n",
    "    idx_priceFilter = np.asarray(idx_priceFilter)\n",
    "    feature=np.delete(feature,idx_priceFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_priceFilter,0)\n",
    "\n",
    "    idx_sqftFilter = np.where(feature[:,12]>3*loc_test[14])\n",
    "    idx_sqftFilter = np.asarray(idx_sqftFilter)\n",
    "    feature=np.delete(feature,idx_sqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_sqftFilter,0)\n",
    "\n",
    "    idx_lotsqftFilter = np.where(feature[:,7]>3*loc_test[9])\n",
    "    idx_lotsqftFilter = np.asarray(idx_lotsqftFilter)\n",
    "    feature=np.delete(feature,idx_lotsqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_lotsqftFilter,0)\n",
    "\n",
    "    idx_numDaysFilter = np.where(feature[:,8] < loc_test[10])\n",
    "    idx_numDaysFilter = np.asarray(idx_numDaysFilter)\n",
    "    feature=np.delete(feature,idx_numDaysFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_numDaysFilter,0)     \n",
    "\n",
    "    feature_test =np.append(loc_test[2:], 0)\n",
    "\n",
    "    if feature.shape[0]==0:\n",
    "        feature = feature_test.reshape((1,feature_test.shape[0]))\n",
    "        Y_train = np.asarray([1])    \n",
    "    feature2 = np.vstack(feature[:, 1:]).astype(np.float) #not using list price as one of the feature\n",
    "    feature_test2 = np.vstack(feature_test[1:]).astype(np.float) #not using list price as one of the feature \n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(feature2)\n",
    "    X_scaled = scaler.transform(feature2)  \n",
    "    X_scaled_normed = preprocessing.normalize(X_scaled, norm='l2')\n",
    "\n",
    "    X_test_scaled = scaler.transform(feature_test2.reshape(1, -1))  \n",
    "    X_test_scaled_normed= preprocessing.normalize(X_test_scaled.reshape(1, -1) , norm='l2')\n",
    "    X_test_scaled_normed = X_test_scaled_normed[:,:]\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(1,interaction_only=True)\n",
    "    X_train = poly.fit_transform(X_scaled_normed)  \n",
    "    X_test = poly.fit_transform(X_test_scaled_normed.reshape(1, -1) )  \n",
    "\n",
    "    for i in range(1,X_train.shape[1]):\n",
    "        col_null = pd.isnull((X_train[:,i]))\n",
    "        idx=np.where(col_null==True)\n",
    "        X_train=np.delete(X_train,idx,0)\n",
    "        Y_train=np.delete(Y_train,idx,0)\n",
    "        col_null2 = pd.isnull((Y_train[:]))\n",
    "        idx2=np.where(col_null2==True)\n",
    "        X_train=np.delete(X_train,idx2,0)\n",
    "        Y_train=np.delete(Y_train,idx2,0)\n",
    "\n",
    "    #print ('the training set size is: [') + str(X_train.shape[0]) +',' + str(X_train.shape[1]) +']'\n",
    "    rf=RandomForestRegressor()\n",
    "    #RR=linear_model.Ridge(alpha =1) \n",
    "    #svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    clf= rf\n",
    "    clf.fit(X_train,Y_train)\n",
    "    diff_est=clf.predict(X_test)\n",
    "    return (diff_est, Y_test)\n",
    "\n",
    "diff_est = []\n",
    "diff_true=[]\n",
    "num_test=0\n",
    "group=0\n",
    "for i in range(0,280000,1000):\n",
    "    diff, Ytrue = pctErr_cal(i)\n",
    "    diff_est.append(diff)\n",
    "    diff_true.append(Ytrue)\n",
    "    num_test=num_test+1\n",
    "    if num_test == 40:\n",
    "        scipy.io.savemat('model1'+str(group)+'_top5000.mat', {'diff_est':diff_est,'diff_true':diff_true})\n",
    "        group=group+1\n",
    "        num_test=num_test-40"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
