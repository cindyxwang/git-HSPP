{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import os\n",
    "from petl import fromcsv, look, cut, tocsv \n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import vincenty\n",
    "from scipy.stats import norm\n",
    "from numpy import linspace\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from pylab import plot,show,hist,figure,title\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from astropy.table import Table, Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load and merge the tables scrapted from public datatbase\n",
    "def p2f(x):\n",
    "    return float(x.strip('%'))/100\n",
    "\n",
    "county2labor = pd.read_csv('CA_county_employment.csv').as_matrix()\n",
    "for i in range(1,county2labor.shape[0]):\n",
    "    county2labor[i,5] = p2f(county2labor[i,5])\n",
    "county2LF=county2labor[:,[0,2,5]]\n",
    "for i in range(1,county2LF.shape[0]):\n",
    "    county2LF[i,1]=int(county2LF[i,1])\n",
    "county2LF[0,2] ='unemploy_rate'\n",
    "\n",
    "propTax = pd.read_csv('CA_propTax.csv').as_matrix()\n",
    "propTax[22,7] = '0'\n",
    "propTax[58,7] = '0'\n",
    "\n",
    "for i in range(1,propTax.shape[0]):\n",
    "    propTax[i,7] = p2f(propTax[i,7])\n",
    "propTax[22,7] = np.median(propTax[:,7])\n",
    "propTax[58,7] = np.median(propTax[:,7])\n",
    "\n",
    "propTax[0,0]='County'\n",
    "propTax = propTax[:,[0,7]]\n",
    "propTax[0,1]='midTax'\n",
    "\n",
    "zip2county = pd.read_csv('CA_zip2county.csv').as_matrix()\n",
    "zip2county = zip2county[:,[0,2]]\n",
    "zip2county[0,0]='zipcd'\n",
    "zip2county[0,1]='County'\n",
    "\n",
    "for i in range(1,zip2county.shape[0]):\n",
    "    zip2county[i,0]=int(zip2county[i,0])\n",
    "\n",
    "\n",
    "df1=pd.DataFrame(county2LF, columns=['County', 'Labor Force', 'unemploy_rate'])\n",
    "df2=pd.DataFrame(propTax, columns=['County', 'midTax'])\n",
    "df3=pd.DataFrame(zip2county, columns=['zipcd', 'County'])\n",
    "\n",
    "['zipcd', 'County','Labor Force', 'unemploy_rate', 'midTax']\n",
    "\n",
    "df_merge=df1.merge(df2,on='County')\n",
    "df_merge_merge=df3.merge(df_merge,on='County')\n",
    "df_zip2countyInfo = df_merge_merge.ix[1:]\n",
    "#df_zip2countyInfo =['County', 'Labor Force', 'unemploy_rate','midTax','zipcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cindy/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning:\n",
      "\n",
      "Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the previous transaction data \n",
    "%matplotlib inline\n",
    "#%qtconsole\n",
    "data = pd.read_csv('soldPend_home_all.csv').as_matrix()\n",
    "\n",
    "low_memory=False\n",
    "mylist=[]\n",
    "u, indices =np.unique(data[:,3], return_index=True)\n",
    "data_uni = data[indices,:]\n",
    "data=data_uni\n",
    "                            \n",
    "\n",
    "idx=[0,1,5,7,18]\n",
    "data[:,idx]=data[:,idx]/1000 #convert price unit to $k\n",
    "data[:,11]=2016-data[:,11] #convert year_built to age\n",
    "\n",
    "idx_forFlip=np.where(data[:,16]==True)\n",
    "data[:,16]=1\n",
    "data[idx_forFlip,16]=2\n",
    "\n",
    "idx_takedown=np.where(data[:,6]==True)\n",
    "data[:,6]=1\n",
    "data[idx_takedown,6]=2\n",
    "\n",
    "\n",
    "#Handeling the non-exist and mis-input zipcode. Currently just not using those data. \n",
    "#It can be modified in the future to get zipcode from latitude and longitude\n",
    "# if those are available.\n",
    "noZip = pd.isnull(data[:,2])\n",
    "idx_noZip=np.where(noZip==True)\n",
    "data = np.delete(data,idx_noZip,0)\n",
    "\n",
    "for i in range(0,data.shape[0]):\n",
    "    if (type(data[i,2]) is str):\n",
    "        #print data[i,2]\n",
    "        data[i,2] = int(data[i,2][:5])\n",
    "        #print data[i,2]\n",
    "idx_misZip=np.where(data[:,2]<90000)   \n",
    "data = np.delete(data,idx_misZip,0)\n",
    "\n",
    "\n",
    "idx_soldyearFilter=np.where(data[:,13]>10000)\n",
    "data=np.delete(data,idx_soldyearFilter,0)\n",
    "idx_lotFilter=np.where(data[:,12]>2*pow(10,11))\n",
    "data=np.delete(data,idx_lotFilter,0)\n",
    "idx_sqftFilter=np.where(data[:,17]>400000)\n",
    "data=np.delete(data,idx_sqftFilter,0)\n",
    "idx_misSqft=np.where(data[:,17]<20)\n",
    "data=np.delete(data,idx_misSqft,0)\n",
    "                 \n",
    "idx_priceFilter=np.where(data[:,18]>200000)\n",
    "data=np.delete(data,idx_priceFilter,0)\n",
    "\n",
    "#handling the unreasonable and irrelevant year_built\n",
    "idx_yearFilter=np.where(data[:,11]<0)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "idx_yearFilter=np.where(data[:,11]>500)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "\n",
    "data_forComp=data\n",
    "\n",
    "col_null = pd.isnull(data[:,1])\n",
    "idx_prepriceExist=np.where(col_null==True)\n",
    "data=np.delete(data,idx_prepriceExist,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imputing #bed and #bath\n",
    "numBath_mis=np.where(data[:,4]>100) #remove an outlier in the numBath\n",
    "data=np.delete(data,numBath_mis,0)\n",
    "\n",
    "noBath = pd.isnull(data[:,4])\n",
    "idx_noBath=np.where(noBath==True)\n",
    "data[idx_noBath,4]=np.median(data[:,4])\n",
    "\n",
    "idx_0Bath = np.where(data[:,4]==0)\n",
    "data[idx_0Bath,4]=np.median(data[:,4])\n",
    "\n",
    "noBed = pd.isnull(data[:,10])\n",
    "idx_noBed=np.where(noBed==True)\n",
    "data[idx_noBed,9]=np.median(data[:,10])\n",
    "\n",
    "idx_0Bed = np.where(data[:,10]==0)\n",
    "data[idx_0Bed,9]=np.median(data[:,10])\n",
    "\n",
    "#drop the rows where there are still features missing\n",
    "for i in range(0,19):\n",
    "    col_null = pd.isnull((data[:,i]))\n",
    "    idx=np.where(col_null==True)\n",
    "    data=np.delete(data,idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge the previous transaction data with transaction data\n",
    "\n",
    "df=pd.DataFrame(data, columns=['pre_sqft_price','pre_price','zipcd','propID','numBath','sqft_price','takedown',\\\n",
    "                               'cur_lis_price','long','lat','numBed','year_built','lot_sqft','numDays','year',\\\n",
    "                               'month','forFlip','sqft','price'])\n",
    "\n",
    "df_statbyZip = df['price'].astype(float).groupby(df['zipcd']).agg([np.median,np.mean,np.std,np.max,np.min,len])\n",
    "\n",
    "data_statbyZip = df_statbyZip.as_matrix()\n",
    "b=(np.unique(data[:,2])).reshape((df_statbyZip.shape[0],1))\n",
    "data_statbyZip=np.append(data_statbyZip,b,1)\n",
    "#df_zip2countyInfo =['County', 'Labor Force', 'unemploy_rate','midTax','zipcd']\n",
    "df_statbyZip=pd.DataFrame(data_statbyZip, columns=['median','mean','std','max','min','count','zipcd'])\n",
    "df_additional = df_statbyZip.merge(df_zip2countyInfo,on='zipcd')\n",
    "df_all=df.merge(df_additional,on='zipcd')\n",
    "data_additional = df_additional.as_matrix()\n",
    "data_all=df_all.as_matrix()\n",
    "data_all[:,[18,28]]=data_all[:,[28,18]]\n",
    "data_all[:,[0,3]]=data_all[:,[3,0]]\n",
    "data_all[:,[1,2]]=data_all[:,[2,1]]\n",
    "data_all[:,[8,9]]=data_all[:,[9,8]]\n",
    "#remove houses that has most likely mis input of sqft and which that has unreasonalbe price per sqft\n",
    "\n",
    "var_noNeed=[3,5,7,20,21,22,23,25]\n",
    "data_model=np.delete(data_all,var_noNeed,1)  #delete the variable that are not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,propID\n",
      "1,zipcd\n",
      "2,pre_price\n",
      "3,pre_sqft_price\n",
      "4,numBath\n",
      "5,sqft_price\n",
      "6,takedown\n",
      "7,cur_lis_price\n",
      "8,long\n",
      "9,lat\n",
      "10,numBed\n",
      "11,year_built\n",
      "12,lot_sqft\n",
      "13,numDays\n",
      "14,year\n",
      "15,month\n",
      "16,forFlip\n",
      "17,sqft\n",
      "18,midTax\n",
      "19,median\n",
      "20,mean\n",
      "21,std\n",
      "22,max\n",
      "23,min\n",
      "24,len\n",
      "25,County\n",
      "26,Labor Force\n",
      "27,unemploy_rate\n",
      "28,price\n"
     ]
    }
   ],
   "source": [
    "# to disply the columes in data_all\n",
    "fields_data_all=['propID','zipcd','pre_price','pre_sqft_price','numBath','sqft_price','takedown','cur_lis_price',\\\n",
    "             'long','lat','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','mean','std','max','min','len','County','Labor Force',\\\n",
    "        'unemploy_rate','price']\n",
    "for index in np.arange(len(fields_data_all)):\n",
    "    print str(index) + \",\" + fields_data_all[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,propID\n",
      "1,zipcd\n",
      "2,pre_price\n",
      "3,numBath\n",
      "4,takedown\n",
      "5,long\n",
      "6,lat\n",
      "7,numBed\n",
      "8,year_built\n",
      "9,lot_sqft\n",
      "10,numDays\n",
      "11,year\n",
      "12,month\n",
      "13,forFlip\n",
      "14,sqft\n",
      "15,midTax\n",
      "16,median\n",
      "17,len\n",
      "18,Labor Force\n",
      "19,unemploy_rate\n",
      "20,price\n"
     ]
    }
   ],
   "source": [
    "# to disply the columes in data_model\n",
    "fields_data_model=['propID','zipcd','pre_price','numBath','takedown',\\\n",
    "             'long','lat','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "for index in np.arange(len(fields_data_model)):\n",
    "    print str(index) + \",\" + fields_data_model[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nzipcd_test=90045\\nidx_zip=np.where(data_additional[:,6]==zipcd_test)\\nloc_test = [0, zipcd_test,1599,3,1, 33.978176,-118.402063,4,2016-1993,5616,0,2016,6,1,2478,data_additional[idx_zip,10],            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\\n\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#enter the info for the house of interest\n",
    "\n",
    "zipcd_test=90012\n",
    "#df_additional=['median','mean','std','max','min','count','zipcd',County','Labor Force', 'unemploy_rate','midTax']\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,850,1,1, 34.065958,-118.246625,2,2016-1958,6976,0,2016,6,1,1196,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n",
    "\n",
    "#fields_data_model=['propID','zipcd','list_price','numBath','takedown',\\\n",
    "             #'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "'''\n",
    "zipcd_test=90039\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,999,2.5,1, 34.086281,-118.247673,4,2016-1990,3743*2,0,2016,6,1,3743,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n",
    "         #'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "'''\n",
    "'''   \n",
    "zipcd_test=90045\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,1599,3,1, 33.978176,-118.402063,4,2016-1993,5616,0,2016,6,1,2478,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated price from model 1 is: 1592.95038\n"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "def price_Est_model1():\n",
    "    Y_train = data_model[:,20]-data_model[:,2]# for sold price, 20; for list price, 2\n",
    "    loc_train = data_model[:,:20]\n",
    "\n",
    "    dist_miles=[]\n",
    "    for i in range(0,loc_train.shape[0]):\n",
    "        dist_miles.append(vincenty(loc_test[5:7], loc_train[i,5:7]).miles)\n",
    "\n",
    "    dist=np.asarray(dist_miles)\n",
    "    dist_sorted = np.sort(dist)\n",
    "\n",
    "    top = 5000\n",
    "    dist_selected = dist_sorted[0:top]\n",
    "\n",
    "    loc_match_id=[np.where(dist == dist_selected[x])[0] for x in range(0,top)]\n",
    "    loc_match_id = np.concatenate(loc_match_id).ravel()\n",
    "    loc_match_id = np.unique(loc_match_id)\n",
    "\n",
    "    feature1=loc_train[loc_match_id,2:]\n",
    "    dist_feature = dist[loc_match_id]\n",
    "    Y_train = Y_train[loc_match_id]\n",
    "\n",
    "    feature=np.vstack((feature1.T, dist_feature.T))\n",
    "    feature=feature.T    \n",
    "\n",
    "    idx_priceFilter = np.where(Y_train[:]>3*loc_test[2])\n",
    "    idx_priceFilter = np.asarray(idx_priceFilter)\n",
    "    feature=np.delete(feature,idx_priceFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_priceFilter,0)\n",
    "\n",
    "    idx_sqftFilter = np.where(feature[:,12]>3*loc_test[14])\n",
    "    idx_sqftFilter = np.asarray(idx_sqftFilter)\n",
    "    feature=np.delete(feature,idx_sqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_sqftFilter,0)\n",
    "\n",
    "    idx_lotsqftFilter = np.where(feature[:,7]>3*loc_test[9])\n",
    "    idx_lotsqftFilter = np.asarray(idx_lotsqftFilter)\n",
    "    feature=np.delete(feature,idx_lotsqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_lotsqftFilter,0)\n",
    "\n",
    "    idx_numDaysFilter = np.where(feature[:,8] < loc_test[10])\n",
    "    idx_numDaysFilter = np.asarray(idx_numDaysFilter)\n",
    "    feature=np.delete(feature,idx_numDaysFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_numDaysFilter,0)     \n",
    "\n",
    "    feature_test =np.append(loc_test[2:], 0)\n",
    "\n",
    "    if feature.shape[0]==0:\n",
    "        feature = feature_test.reshape((1,feature_test.shape[0]))\n",
    "        Y_train = np.asarray([1])    \n",
    "    feature2 = np.vstack(feature[:, 1:]).astype(np.float) #to not using list price as one of the feature\n",
    "    feature_test2 = np.vstack(feature_test[1:]).astype(np.float) #to not using list price as one of the feature\n",
    "\n",
    "    #feature2 = np.vstack(feature[:,:]).astype(np.float) #to use list price as one of the feature\n",
    "    #feature_test2 = np.vstack(feature_test).astype(np.float) #to use list price as one of the feature\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(feature2)\n",
    "    X_scaled = scaler.transform(feature2)  \n",
    "    X_scaled_normed = preprocessing.normalize(X_scaled, norm='l2')\n",
    "\n",
    "    X_test_scaled = scaler.transform(feature_test2.reshape(1, -1))  \n",
    "    X_test_scaled_normed= preprocessing.normalize(X_test_scaled.reshape(1, -1) , norm='l2')\n",
    "    X_test_scaled_normed = X_test_scaled_normed[:,:]\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(1,interaction_only=True)\n",
    "    X_train = poly.fit_transform(X_scaled_normed)  \n",
    "    X_test = poly.fit_transform(X_test_scaled_normed.reshape(1, -1) )  \n",
    "\n",
    "    for i in range(1,X_train.shape[1]):\n",
    "        col_null = pd.isnull((X_train[:,i]))\n",
    "        idx=np.where(col_null==True)\n",
    "        X_train=np.delete(X_train,idx,0)\n",
    "        Y_train=np.delete(Y_train,idx,0)\n",
    "        col_null2 = pd.isnull((Y_train[:]))\n",
    "        idx2=np.where(col_null2==True)\n",
    "        X_train=np.delete(X_train,idx2,0)\n",
    "        Y_train=np.delete(Y_train,idx2,0)\n",
    "\n",
    "    #print ('the training set size is: [') + str(X_train.shape[0]) +',' + str(X_train.shape[1]) +']'\n",
    "    rf=RandomForestRegressor()\n",
    "    #RR=linear_model.Ridge(alpha =1) \n",
    "    #svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    clf= rf\n",
    "    clf.fit(X_train,Y_train)\n",
    "    diff_est=clf.predict(X_test)\n",
    "    price_est=diff_est+loc_test[2]\n",
    "    return price_est\n",
    "\n",
    "price_est_model1=[]\n",
    "for i in range(0,10,1):\n",
    "    price_est=price_Est_model1()\n",
    "    price_est_model1.append(price_est)\n",
    "\n",
    "print ('the estimated price from model 1 is: ') + str(np.mean(price_est_model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 houses are used for comparable house search\n",
      "the estimated price from model 2 is: 838.924479167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "&lt;Table length=10&gt;\n",
       "<table id=\"table4922033104\">\n",
       "<thead><tr><th>similarity [1]</th><th>propID</th><th>zipcd</th><th>#Bath</th><th>#Bed</th><th>yr._built</th><th>lot</th><th>sold_yr.</th><th>sold_mon.</th><th>sqft</th><th>prop_taxrate</th><th>dist</th><th>sold_price</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>object</th><th>object</th><th>uint8</th><th>uint8</th><th>uint16</th><th>float64</th><th>uint16</th><th>uint8</th><th>float64</th><th>float64</th><th>float64</th><th>object</th></tr></thead>\n",
       "<tr><td>0.41</td><td>5666513e6b16b194960000d9</td><td>90026</td><td>2</td><td>2</td><td>1913</td><td>5986.0</td><td>2016</td><td>5</td><td>1092.0</td><td>0.0129</td><td>1.3</td><td>895.0</td></tr>\n",
       "<tr><td>0.51</td><td>56cfa3ce6b16b1380ee7ce2f</td><td>90026</td><td>2</td><td>3</td><td>1926</td><td>7826.0</td><td>2016</td><td>6</td><td>1134.0</td><td>0.0129</td><td>0.7</td><td>795.0</td></tr>\n",
       "<tr><td>0.38</td><td>56fd68c7ec0c19f4218fb4e0</td><td>90026</td><td>1</td><td>2</td><td>1952</td><td>3431.0</td><td>2016</td><td>5</td><td>1320.0</td><td>0.0129</td><td>2.1</td><td>920.0</td></tr>\n",
       "<tr><td>0.31</td><td>57025f28ec0c191c7ba37f3a</td><td>90026</td><td>1</td><td>2</td><td>1925</td><td>3000.0</td><td>2016</td><td>5</td><td>1110.0</td><td>0.0129</td><td>2.1</td><td>785.0</td></tr>\n",
       "<tr><td>0.27</td><td>5704f837ec0c19ef078826a2</td><td>90026</td><td>2</td><td>3</td><td>1915</td><td>4360.0</td><td>2016</td><td>5</td><td>1300.0</td><td>0.0129</td><td>1.6</td><td>799.0</td></tr>\n",
       "<tr><td>0.55</td><td>570d8632ec0c19f69f22bcd0</td><td>90026</td><td>1</td><td>2</td><td>1953</td><td>11040.0</td><td>2016</td><td>6</td><td>1164.0</td><td>0.0129</td><td>1.5</td><td>862.0</td></tr>\n",
       "<tr><td>0.29</td><td>55b17dc169702d13bd000041</td><td>90039</td><td>2</td><td>2</td><td>1950</td><td>4956.0</td><td>2016</td><td>5</td><td>1316.0</td><td>0.0129</td><td>2.6</td><td>895.0</td></tr>\n",
       "<tr><td>0.27</td><td>5637e9f66b16b11653000233</td><td>90039</td><td>2</td><td>3</td><td>1925</td><td>4999.0</td><td>2016</td><td>6</td><td>1250.0</td><td>0.0129</td><td>2.4</td><td>785.0</td></tr>\n",
       "<tr><td>0.34</td><td>56df14006b16b155d785edd5</td><td>90039</td><td>1</td><td>2</td><td>1937</td><td>4728.0</td><td>2016</td><td>5</td><td>1044.0</td><td>0.0129</td><td>2.6</td><td>845.0</td></tr>\n",
       "<tr><td>0.51</td><td>56674fa46b16b12499000110</td><td>90029</td><td>1</td><td>2</td><td>1922</td><td>4950.0</td><td>2016</td><td>3</td><td>1134.0</td><td>0.0129</td><td>2.6</td><td>799.0</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=10>\n",
       "similarity [1]          propID          zipcd  ...   dist  sold_price\n",
       "   float64              object          object ... float64   object  \n",
       "-------------- ------------------------ ------ ... ------- ----------\n",
       "          0.41 5666513e6b16b194960000d9  90026 ...     1.3      895.0\n",
       "          0.51 56cfa3ce6b16b1380ee7ce2f  90026 ...     0.7      795.0\n",
       "          0.38 56fd68c7ec0c19f4218fb4e0  90026 ...     2.1      920.0\n",
       "          0.31 57025f28ec0c191c7ba37f3a  90026 ...     2.1      785.0\n",
       "          0.27 5704f837ec0c19ef078826a2  90026 ...     1.6      799.0\n",
       "          0.55 570d8632ec0c19f69f22bcd0  90026 ...     1.5      862.0\n",
       "          0.29 55b17dc169702d13bd000041  90039 ...     2.6      895.0\n",
       "          0.27 5637e9f66b16b11653000233  90039 ...     2.4      785.0\n",
       "          0.34 56df14006b16b155d785edd5  90039 ...     2.6      845.0\n",
       "          0.51 56674fa46b16b12499000110  90029 ...     2.6      799.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 2\n",
    "\n",
    "Y_train_compa = data_model[:,20]\n",
    "feature1_compa=data_model[:,:20]\n",
    "\n",
    "idx_numDaysFilter_compa = np.where(feature1_compa[:,10]>(loc_test[10]+180))\n",
    "idx_numDaysFilter_compa = np.asarray(idx_numDaysFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_numDaysFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter_compa,0)\n",
    "\n",
    "idx_numDaysFilter2_compa = np.where(feature1_compa[:,10] < loc_test[10])\n",
    "idx_numDaysFilter2_compa = np.asarray(idx_numDaysFilter2_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_numDaysFilter2_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter2_compa,0)   \n",
    "\n",
    "idx_priceFilter_compa = np.where(np.absolute(np.divide(Y_train_compa-loc_test[2],loc_test[2]))>0.1)\n",
    "idx_priceFilter_compa = np.asarray(idx_priceFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_priceFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_priceFilter_compa,0)\n",
    "\n",
    "idx_sqftFilter_compa = np.where(np.abs(np.divide(feature1_compa[:,14]-loc_test[14],loc_test[14]))>0.2)\n",
    "idx_sqftFilter_compa = np.asarray(idx_sqftFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_sqftFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_sqftFilter_compa,0)\n",
    "\n",
    "idx_flipFilter_compa = np.where(feature1_compa[:,13] != loc_test[13])\n",
    "idx_flipFilter_compa = np.asarray(idx_flipFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_flipFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_flipFilter_compa,0)\n",
    "\n",
    "idx_tdFilter_compa = np.where(feature1_compa[:,4] != loc_test[4])\n",
    "idx_tdFilter_compa = np.asarray(idx_tdFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_tdFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_tdFilter_compa,0)\n",
    "\n",
    "dist_compa=[]\n",
    "for i in range(0,feature1_compa.shape[0]):\n",
    "    dist_compa.append(vincenty(loc_test[5:7], feature1_compa[i,5:7]).miles)\n",
    "    \n",
    "dist_compa=np.asarray(dist_compa)\n",
    "dist_compa_sorted = np.sort(dist_compa)\n",
    "\n",
    "top_compa= 30\n",
    "dist_select_compa= dist_compa_sorted[0:top_compa]\n",
    "loc_match_id_compa=[np.where(dist_compa == dist_select_compa[x])[0] for x in range(0,top_compa)]\n",
    "loc_match_id_compa = np.concatenate(loc_match_id_compa).ravel()\n",
    "loc_match_id_compa = np.unique(loc_match_id_compa)\n",
    "dist_selected_compa = dist_compa_sorted[0:top_compa]\n",
    "dist_feature_compa = dist_compa[loc_match_id_compa]\n",
    "Y_train_compa = Y_train_compa[loc_match_id_compa]\n",
    "feature_test_compa =np.append(loc_test, 0)\n",
    "\n",
    "feature1_compa = feature1_compa[loc_match_id_compa,:]\n",
    "feature_compa=np.vstack((feature1_compa.T, dist_feature_compa.T))\n",
    "feature_compa=feature_compa.T\n",
    "\n",
    "feature_compa = np.vstack(feature_compa[:, 3:]).astype(np.float)\n",
    "feature_test_compa = np.vstack(feature_test_compa[3:]).astype(np.float)\n",
    "\n",
    "scaler_compa = preprocessing.StandardScaler().fit(feature_compa)\n",
    "X_scaled_compa = scaler_compa.transform(feature_compa)  \n",
    "X_scaled_normed_compa = preprocessing.normalize(X_scaled_compa, norm='l2')\n",
    "\n",
    "X_train_compa = X_scaled_normed_compa\n",
    "\n",
    "X_test_scaled_compa = scaler_compa.transform(feature_test_compa.reshape(1, -1))  \n",
    "X_test_scaled_normed_compa= preprocessing.normalize(X_test_scaled_compa.reshape(1, -1) , norm='l2')\n",
    "X_test_compa = X_test_scaled_normed_compa\n",
    "\n",
    "print str(X_train_compa.shape[0]) + (' houses are used for comparable house search')\n",
    "\n",
    "import math\n",
    "from itertools import izip\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return sum(map(lambda x: x[0] * x[1], izip(v1, v2)))\n",
    "\n",
    "def cosine_measure(v1, v2):\n",
    "    prod = dot_product(v1, v2)\n",
    "    len1 = math.sqrt(dot_product(v1, v1))\n",
    "    len2 = math.sqrt(dot_product(v2, v2))\n",
    "    return prod / (len1 * len2)\n",
    "\n",
    "similarity = []\n",
    "for i in range(0,X_train_compa.shape[0]):\n",
    "    similarity.append(cosine_measure(X_test_compa.reshape((-1,1)), X_train_compa[i,:].reshape((-1,1))) )\n",
    "\n",
    "    \n",
    "    \n",
    "# find the top 10 similar houses based on cosine similarity \n",
    "from astropy.table import Table, Column\n",
    "top_simi = 10\n",
    "\n",
    "simi_sorted = sorted(similarity, reverse=True)\n",
    "simi_select = simi_sorted[0:top_simi]\n",
    "simi_match_id =[np.where(similarity == simi_select[x])[0] for x in range(0,top_simi)]\n",
    "simi_match_id = np.concatenate(simi_match_id).ravel()\n",
    "simi_match_id = np.unique(simi_match_id)\n",
    "\n",
    "numBath = feature_compa[simi_match_id, 0].astype(np.uint8)\n",
    "numBed = feature_compa[simi_match_id, 4].astype(np.uint8) \n",
    "year_built = 2016-feature_compa[simi_match_id, 5].astype(np.uint16) \n",
    "lot_sqft = feature_compa[simi_match_id, 6] \n",
    "sold_year = feature_compa[simi_match_id, 8].astype(np.uint16) \n",
    "sold_month =feature_compa[simi_match_id, 9].astype(np.uint8) \n",
    "sqft=feature_compa[simi_match_id, 11] \n",
    "prop_tax_rate = feature_compa[simi_match_id, 12] \n",
    "count_perzip = feature_compa[simi_match_id, 14].astype(np.uint16) \n",
    "distance=feature_compa[simi_match_id, 17]\n",
    "zipcd = feature1_compa[simi_match_id, 1]\n",
    "propID = feature1_compa[simi_match_id, 0]\n",
    "\n",
    "\n",
    "simi = np.asarray(similarity)[simi_match_id]\n",
    "\n",
    "for i in range(0,simi_match_id.shape[0]):\n",
    "    simi[i]=float(int(simi[i]*100))/100\n",
    "    distance[i]=float(int(distance[i]*10))/10\n",
    "    \n",
    "simi=simi.reshape((-1,1))    \n",
    "sold_price = Y_train_compa[simi_match_id]\n",
    "\n",
    "t = Table([simi,propID, zipcd, numBath, numBed,year_built,lot_sqft,sold_year,sold_month,sqft,\\\n",
    "           prop_tax_rate,distance,sold_price], names=('similarity','propID','zipcd','#Bath', '#Bed',\\\n",
    "                                                            'yr._built','lot','sold_yr.',\\\n",
    "                                                            'sold_mon.','sqft','prop_taxrate','dist',\\\n",
    "                                                            'sold_price'))\n",
    "weight = np.asarray(simi/np.sum(simi)).reshape((10,))\n",
    "price_est_weighted=np.sum(np.multiply(sold_price,weight))\n",
    "price_est_median=np.median(sold_price)\n",
    "print ('the estimated price from model 2 is: ') + str(price_est_weighted)\n",
    "\n",
    "t_propID = Table([propID,year_built], names=('propID','year_built'))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('top10comp_#825.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in t_propID]\n",
    "\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
