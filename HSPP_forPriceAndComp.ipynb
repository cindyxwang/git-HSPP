{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "import os\n",
    "from petl import fromcsv, look, cut, tocsv \n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import vincenty\n",
    "from scipy.stats import norm\n",
    "from numpy import linspace\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from pylab import plot,show,hist,figure,title\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from astropy.table import Table, Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load and merge the tables scrapted from public datatbase\n",
    "def p2f(x):\n",
    "    return float(x.strip('%'))/100\n",
    "\n",
    "county2labor = pd.read_csv('CA_county_employment.csv').as_matrix()\n",
    "for i in range(1,county2labor.shape[0]):\n",
    "    county2labor[i,5] = p2f(county2labor[i,5])\n",
    "county2LF=county2labor[:,[0,2,5]]\n",
    "for i in range(1,county2LF.shape[0]):\n",
    "    county2LF[i,1]=int(county2LF[i,1])\n",
    "county2LF[0,2] ='unemploy_rate'\n",
    "\n",
    "propTax = pd.read_csv('CA_propTax.csv').as_matrix()\n",
    "propTax[22,7] = '0'\n",
    "propTax[58,7] = '0'\n",
    "\n",
    "for i in range(1,propTax.shape[0]):\n",
    "    propTax[i,7] = p2f(propTax[i,7])\n",
    "propTax[22,7] = np.median(propTax[:,7])\n",
    "propTax[58,7] = np.median(propTax[:,7])\n",
    "\n",
    "propTax[0,0]='County'\n",
    "propTax = propTax[:,[0,7]]\n",
    "propTax[0,1]='midTax'\n",
    "\n",
    "zip2county = pd.read_csv('CA_zip2county.csv').as_matrix()\n",
    "zip2county = zip2county[:,[0,2]]\n",
    "zip2county[0,0]='zipcd'\n",
    "zip2county[0,1]='County'\n",
    "\n",
    "for i in range(1,zip2county.shape[0]):\n",
    "    zip2county[i,0]=int(zip2county[i,0])\n",
    "\n",
    "\n",
    "df1=pd.DataFrame(county2LF, columns=['County', 'Labor Force', 'unemploy_rate'])\n",
    "df2=pd.DataFrame(propTax, columns=['County', 'midTax'])\n",
    "df3=pd.DataFrame(zip2county, columns=['zipcd', 'County'])\n",
    "\n",
    "['zipcd', 'County','Labor Force', 'unemploy_rate', 'midTax']\n",
    "\n",
    "df_merge=df1.merge(df2,on='County')\n",
    "df_merge_merge=df3.merge(df_merge,on='County')\n",
    "df_zip2countyInfo = df_merge_merge.ix[1:]\n",
    "#df_zip2countyInfo =['County', 'Labor Force', 'unemploy_rate','midTax','zipcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the previous transaction data \n",
    "%matplotlib inline\n",
    "#%qtconsole\n",
    "#data = pd.read_csv('soldPend_home_all.csv').as_matrix()\n",
    "data = pd.read_csv('sold_home_all.csv').as_matrix()\n",
    "\n",
    "low_memory=False\n",
    "mylist=[]\n",
    "u, indices =np.unique(data[:,3], return_index=True)\n",
    "data_uni = data[indices,:]\n",
    "data=data_uni\n",
    "                            \n",
    "\n",
    "idx=[0,1,5,7,18]\n",
    "data[:,idx]=data[:,idx]/1000 #convert price unit to $k\n",
    "data[:,11]=2016-data[:,11] #convert year_built to age\n",
    "\n",
    "idx_forFlip=np.where(data[:,16]==True)\n",
    "data[:,16]=1\n",
    "data[idx_forFlip,16]=2\n",
    "\n",
    "idx_takedown=np.where(data[:,6]==True)\n",
    "data[:,6]=1\n",
    "data[idx_takedown,6]=2\n",
    "\n",
    "\n",
    "#Handeling the non-exist and mis-input zipcode. Currently just not using those data. \n",
    "#It can be modified in the future to get zipcode from latitude and longitude\n",
    "# if those are available.\n",
    "noZip = pd.isnull(data[:,2])\n",
    "idx_noZip=np.where(noZip==True)\n",
    "data = np.delete(data,idx_noZip,0)\n",
    "\n",
    "for i in range(0,data.shape[0]):\n",
    "    if (type(data[i,2]) is str):\n",
    "        #print data[i,2]\n",
    "        data[i,2] = int(data[i,2][:5])\n",
    "        #print data[i,2]\n",
    "idx_misZip=np.where(data[:,2]<90000)   \n",
    "data = np.delete(data,idx_misZip,0)\n",
    "\n",
    "\n",
    "idx_soldyearFilter=np.where(data[:,13]>10000)\n",
    "data=np.delete(data,idx_soldyearFilter,0)\n",
    "idx_lotFilter=np.where(data[:,12]>2*pow(10,11))\n",
    "data=np.delete(data,idx_lotFilter,0)\n",
    "idx_sqftFilter=np.where(data[:,17]>400000)\n",
    "data=np.delete(data,idx_sqftFilter,0)\n",
    "idx_misSqft=np.where(data[:,17]<20)\n",
    "data=np.delete(data,idx_misSqft,0)\n",
    "                 \n",
    "idx_priceFilter=np.where(data[:,18]>200000)\n",
    "data=np.delete(data,idx_priceFilter,0)\n",
    "\n",
    "#handling the unreasonable and irrelevant year_built\n",
    "idx_yearFilter=np.where(data[:,11]<0)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "idx_yearFilter=np.where(data[:,11]>500)\n",
    "data[idx_yearFilter,11]=np.median(data[:,11])\n",
    "\n",
    "data_forComp=data\n",
    "\n",
    "col_null = pd.isnull(data[:,1])\n",
    "idx_prepriceExist=np.where(col_null==True)\n",
    "data=np.delete(data,idx_prepriceExist,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imputing #bed and #bath\n",
    "numBath_mis=np.where(data[:,4]>100) #remove an outlier in the numBath\n",
    "data=np.delete(data,numBath_mis,0)\n",
    "\n",
    "noBath = pd.isnull(data[:,4])\n",
    "idx_noBath=np.where(noBath==True)\n",
    "data[idx_noBath,4]=np.median(data[:,4])\n",
    "\n",
    "idx_0Bath = np.where(data[:,4]==0)\n",
    "data[idx_0Bath,4]=np.median(data[:,4])\n",
    "\n",
    "noBed = pd.isnull(data[:,10])\n",
    "idx_noBed=np.where(noBed==True)\n",
    "data[idx_noBed,9]=np.median(data[:,10])\n",
    "\n",
    "idx_0Bed = np.where(data[:,10]==0)\n",
    "data[idx_0Bed,9]=np.median(data[:,10])\n",
    "\n",
    "#drop the rows where there are still features missing\n",
    "for i in range(0,19):\n",
    "    col_null = pd.isnull((data[:,i]))\n",
    "    idx=np.where(col_null==True)\n",
    "    data=np.delete(data,idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge the previous transaction data with transaction data\n",
    "\n",
    "df=pd.DataFrame(data, columns=['pre_sqft_price','pre_price','zipcd','propID','numBath','sqft_price','takedown',\\\n",
    "                               'cur_lis_price','long','lat','numBed','year_built','lot_sqft','numDays','year',\\\n",
    "                               'month','forFlip','sqft','price'])\n",
    "\n",
    "df_statbyZip = df['price'].astype(float).groupby(df['zipcd']).agg([np.median,np.mean,np.std,np.max,np.min,len])\n",
    "\n",
    "data_statbyZip = df_statbyZip.as_matrix()\n",
    "b=(np.unique(data[:,2])).reshape((df_statbyZip.shape[0],1))\n",
    "data_statbyZip=np.append(data_statbyZip,b,1)\n",
    "#df_zip2countyInfo =['County', 'Labor Force', 'unemploy_rate','midTax','zipcd']\n",
    "df_statbyZip=pd.DataFrame(data_statbyZip, columns=['median','mean','std','max','min','count','zipcd'])\n",
    "df_additional = df_statbyZip.merge(df_zip2countyInfo,on='zipcd')\n",
    "df_all=df.merge(df_additional,on='zipcd')\n",
    "data_additional = df_additional.as_matrix()\n",
    "data_all=df_all.as_matrix()\n",
    "data_all[:,[18,28]]=data_all[:,[28,18]]\n",
    "data_all[:,[0,3]]=data_all[:,[3,0]]\n",
    "data_all[:,[1,2]]=data_all[:,[2,1]]\n",
    "data_all[:,[8,9]]=data_all[:,[9,8]]\n",
    "#remove houses that has most likely mis input of sqft and which that has unreasonalbe price per sqft\n",
    "\n",
    "var_noNeed=[3,5,7,20,21,22,23,25]\n",
    "data_model=np.delete(data_all,var_noNeed,1)  #delete the variable that are not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_all\n",
    "fields_data_all=['propID','zipcd','pre_price','pre_sqft_price','numBath','sqft_price','takedown','cur_lis_price',\\\n",
    "             'long','lat','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','mean','std','max','min','len','County','Labor Force',\\\n",
    "        'unemploy_rate','price']\n",
    "#for index in np.arange(len(fields_data_all)):\n",
    "    #print str(index) + \",\" + fields_data_all[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to disply the columes in data_model\n",
    "fields_data_model=['propID','zipcd','pre_price','numBath','takedown',\\\n",
    "             'long','lat','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "             'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "#for index in np.arange(len(fields_data_model)):\n",
    "    #print str(index) + \",\" + fields_data_model[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#enter the info for the house of interest\n",
    "         #'forFlip','sqft','midTax','median','len','Labor Force','unemploy_rate','price']\n",
    "'''\n",
    "zipcd_test=90012\n",
    "#df_additional=['median','mean','std','max','min','count','zipcd',County','Labor Force', 'unemploy_rate','midTax']\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,850,1,1, 34.065958,-118.246625,2,2016-1958,6976,0,2016,6,1,1196,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n",
    "'''\n",
    "#fields_data_model=['propID','zipcd','list_price','numBath','takedown',\\\n",
    "             #'lat','long','numBed','year_built','lot_sqft','numDays','year','month',\\\n",
    "\n",
    "'''\n",
    "zipcd_test=90039\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,999,2.5,1, 34.086281,-118.247673,4,2016-1990,3743*2,0,2016,6,1,3743,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n",
    "\n",
    "''' \n",
    "zipcd_test=90045\n",
    "idx_zip=np.where(data_additional[:,6]==zipcd_test)\n",
    "loc_test = [0, zipcd_test,1599,3,1, 33.978176,-118.402063,4,2016-1993,5616,0,2016,6,1,2478,data_additional[idx_zip,10],\\\n",
    "            data_additional[idx_zip,0],data_additional[idx_zip,5],data_additional[idx_zip,8], data_additional[idx_zip,9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 1\n",
    "def price_Est_model1():\n",
    "    Y_train = data_model[:,20]-data_model[:,2]# for sold price, 20; for list price, 2\n",
    "    loc_train = data_model[:,:20]\n",
    "\n",
    "    dist_miles=[]\n",
    "    for i in range(0,loc_train.shape[0]):\n",
    "        dist_miles.append(vincenty(loc_test[5:7], loc_train[i,5:7]).miles)\n",
    "\n",
    "    dist=np.asarray(dist_miles)\n",
    "    dist_sorted = np.sort(dist)\n",
    "\n",
    "    top = 5000\n",
    "    dist_selected = dist_sorted[0:top]\n",
    "\n",
    "    loc_match_id=[np.where(dist == dist_selected[x])[0] for x in range(0,top)]\n",
    "    loc_match_id = np.concatenate(loc_match_id).ravel()\n",
    "    loc_match_id = np.unique(loc_match_id)\n",
    "\n",
    "    feature1=loc_train[loc_match_id,2:]\n",
    "    dist_feature = dist[loc_match_id]\n",
    "    Y_train = Y_train[loc_match_id]\n",
    "\n",
    "    feature=np.vstack((feature1.T, dist_feature.T))\n",
    "    feature=feature.T    \n",
    "\n",
    "    idx_priceFilter = np.where(Y_train[:]>3*loc_test[2])\n",
    "    idx_priceFilter = np.asarray(idx_priceFilter)\n",
    "    feature=np.delete(feature,idx_priceFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_priceFilter,0)\n",
    "\n",
    "    idx_sqftFilter = np.where(feature[:,12]>3*loc_test[14])\n",
    "    idx_sqftFilter = np.asarray(idx_sqftFilter)\n",
    "    feature=np.delete(feature,idx_sqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_sqftFilter,0)\n",
    "\n",
    "    idx_lotsqftFilter = np.where(feature[:,7]>3*loc_test[9])\n",
    "    idx_lotsqftFilter = np.asarray(idx_lotsqftFilter)\n",
    "    feature=np.delete(feature,idx_lotsqftFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_lotsqftFilter,0)\n",
    "\n",
    "    idx_numDaysFilter = np.where(feature[:,8] < loc_test[10])\n",
    "    idx_numDaysFilter = np.asarray(idx_numDaysFilter)\n",
    "    feature=np.delete(feature,idx_numDaysFilter,0)\n",
    "    Y_train=np.delete(Y_train,idx_numDaysFilter,0)     \n",
    "\n",
    "    feature_test =np.append(loc_test[2:], 0)\n",
    "\n",
    "    if feature.shape[0]==0:\n",
    "        feature = feature_test.reshape((1,feature_test.shape[0]))\n",
    "        Y_train = np.asarray([1])    \n",
    "    feature2 = np.vstack(feature[:, 1:]).astype(np.float) #to not using list price as one of the feature\n",
    "    feature_test2 = np.vstack(feature_test[1:]).astype(np.float) #to not using list price as one of the feature\n",
    "\n",
    "    #feature2 = np.vstack(feature[:,:]).astype(np.float) #to use list price as one of the feature\n",
    "    #feature_test2 = np.vstack(feature_test).astype(np.float) #to use list price as one of the feature\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(feature2)\n",
    "    X_scaled = scaler.transform(feature2)  \n",
    "    X_scaled_normed = preprocessing.normalize(X_scaled, norm='l2')\n",
    "\n",
    "    X_test_scaled = scaler.transform(feature_test2.reshape(1, -1))  \n",
    "    X_test_scaled_normed= preprocessing.normalize(X_test_scaled.reshape(1, -1) , norm='l2')\n",
    "    X_test_scaled_normed = X_test_scaled_normed[:,:]\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(1,interaction_only=True)\n",
    "    X_train = poly.fit_transform(X_scaled_normed)  \n",
    "    X_test = poly.fit_transform(X_test_scaled_normed.reshape(1, -1) )  \n",
    "\n",
    "    for i in range(1,X_train.shape[1]):\n",
    "        col_null = pd.isnull((X_train[:,i]))\n",
    "        idx=np.where(col_null==True)\n",
    "        X_train=np.delete(X_train,idx,0)\n",
    "        Y_train=np.delete(Y_train,idx,0)\n",
    "        col_null2 = pd.isnull((Y_train[:]))\n",
    "        idx2=np.where(col_null2==True)\n",
    "        X_train=np.delete(X_train,idx2,0)\n",
    "        Y_train=np.delete(Y_train,idx2,0)\n",
    "\n",
    "    #print ('the training set size is: [') + str(X_train.shape[0]) +',' + str(X_train.shape[1]) +']'\n",
    "    rf=RandomForestRegressor()\n",
    "    #RR=linear_model.Ridge(alpha =1) \n",
    "    #svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    clf= rf\n",
    "    clf.fit(X_train,Y_train)\n",
    "    diff_est=clf.predict(X_test)\n",
    "    price_est=diff_est+loc_test[2]\n",
    "    return price_est\n",
    "\n",
    "price_est_model1=[]\n",
    "for i in range(0,10,1):\n",
    "    price_est=price_Est_model1()\n",
    "    price_est_model1.append(price_est)\n",
    "\n",
    "print ('the estimated price from model 1 is: ') + str(np.mean(price_est_model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 houses are used for comparable house search\n",
      "the estimated price from model 2 is: 1556.18940391\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "&lt;Table length=10&gt;\n",
       "<table id=\"table4815196624\">\n",
       "<thead><tr><th>similarity [1]</th><th>propID</th><th>zipcd</th><th>#Bath</th><th>#Bed</th><th>yr._built</th><th>lot</th><th>sold_yr.</th><th>sold_mon.</th><th>sqft</th><th>prop_taxrate</th><th>dist</th><th>sold_price</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>object</th><th>object</th><th>uint8</th><th>uint8</th><th>uint16</th><th>float64</th><th>uint16</th><th>uint8</th><th>float64</th><th>float64</th><th>float64</th><th>object</th></tr></thead>\n",
       "<tr><td>0.62</td><td>55d35b446b16b1ef08000028</td><td>90045</td><td>3</td><td>5</td><td>1942</td><td>5500.0</td><td>2016</td><td>2</td><td>2600.0</td><td>0.0129</td><td>1.6</td><td>1499.0</td></tr>\n",
       "<tr><td>0.99</td><td>564e1b486b16b161260000f2</td><td>90045</td><td>3</td><td>4</td><td>1993</td><td>7800.0</td><td>2016</td><td>1</td><td>2478.0</td><td>0.0129</td><td>0.1</td><td>1752.555</td></tr>\n",
       "<tr><td>0.79</td><td>56aabec36b16b19087ddeb20</td><td>90045</td><td>3</td><td>3</td><td>1948</td><td>8429.0</td><td>2016</td><td>2</td><td>2319.0</td><td>0.0129</td><td>0.7</td><td>1551.5</td></tr>\n",
       "<tr><td>0.81</td><td>56c65b696b16b14d4c0f30b2</td><td>90045</td><td>3</td><td>5</td><td>2016</td><td>7090.0</td><td>2016</td><td>5</td><td>2694.0</td><td>0.0129</td><td>1.2</td><td>1499.0</td></tr>\n",
       "<tr><td>0.77</td><td>56f5eae8ec0c193730c13d99</td><td>90230</td><td>2</td><td>4</td><td>1999</td><td>4240.0</td><td>2016</td><td>5</td><td>2252.0</td><td>0.0129</td><td>1.4</td><td>1450.0</td></tr>\n",
       "<tr><td>0.31</td><td>563bd1d06b16b1309900028e</td><td>90293</td><td>4</td><td>5</td><td>1960</td><td>5004.0</td><td>2016</td><td>3</td><td>3470.0</td><td>0.0129</td><td>2.0</td><td>1500.0</td></tr>\n",
       "<tr><td>0.4</td><td>56b40f526b16b163d660be70</td><td>90232</td><td>3</td><td>4</td><td>2007</td><td>6439.0</td><td>2016</td><td>4</td><td>2226.0</td><td>0.0129</td><td>2.3</td><td>1650.0</td></tr>\n",
       "<tr><td>0.36</td><td>55f747286b16b1ca39000002</td><td>90066</td><td>2</td><td>4</td><td>2001</td><td>5170.0</td><td>2016</td><td>3</td><td>3170.0</td><td>0.0129</td><td>2.2</td><td>1567.0</td></tr>\n",
       "<tr><td>0.43</td><td>55fb50ad6b16b1ca2900004b</td><td>90066</td><td>2</td><td>3</td><td>2015</td><td>5022.0</td><td>2016</td><td>5</td><td>1950.0</td><td>0.0129</td><td>1.7</td><td>1450.0</td></tr>\n",
       "<tr><td>0.14</td><td>57292947ec0c1928317a8b60</td><td>90094</td><td>3</td><td>3</td><td>2004</td><td>80446.0</td><td>2016</td><td>5</td><td>2314.0</td><td>0.0129</td><td>1.1</td><td>1517.0</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=10>\n",
       "similarity [1]          propID          zipcd  ...   dist  sold_price\n",
       "   float64              object          object ... float64   object  \n",
       "-------------- ------------------------ ------ ... ------- ----------\n",
       "          0.62 55d35b446b16b1ef08000028  90045 ...     1.6     1499.0\n",
       "          0.99 564e1b486b16b161260000f2  90045 ...     0.1   1752.555\n",
       "          0.79 56aabec36b16b19087ddeb20  90045 ...     0.7     1551.5\n",
       "          0.81 56c65b696b16b14d4c0f30b2  90045 ...     1.2     1499.0\n",
       "          0.77 56f5eae8ec0c193730c13d99  90230 ...     1.4     1450.0\n",
       "          0.31 563bd1d06b16b1309900028e  90293 ...     2.0     1500.0\n",
       "           0.4 56b40f526b16b163d660be70  90232 ...     2.3     1650.0\n",
       "          0.36 55f747286b16b1ca39000002  90066 ...     2.2     1567.0\n",
       "          0.43 55fb50ad6b16b1ca2900004b  90066 ...     1.7     1450.0\n",
       "          0.14 57292947ec0c1928317a8b60  90094 ...     1.1     1517.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 2\n",
    "\n",
    "Y_train_compa = data_model[:,20]\n",
    "feature1_compa=data_model[:,:20]\n",
    "\n",
    "idx_numDaysFilter_compa = np.where(feature1_compa[:,10]>(loc_test[10]+180))\n",
    "idx_numDaysFilter_compa = np.asarray(idx_numDaysFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_numDaysFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter_compa,0)\n",
    "\n",
    "idx_numDaysFilter2_compa = np.where(feature1_compa[:,10] < loc_test[10])\n",
    "idx_numDaysFilter2_compa = np.asarray(idx_numDaysFilter2_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_numDaysFilter2_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_numDaysFilter2_compa,0)   \n",
    "\n",
    "idx_priceFilter_compa = np.where(np.absolute(np.divide(Y_train_compa-loc_test[2],loc_test[2]))>0.1)\n",
    "idx_priceFilter_compa = np.asarray(idx_priceFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_priceFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_priceFilter_compa,0)\n",
    "\n",
    "idx_sqftFilter_compa = np.where(np.abs(np.divide(feature1_compa[:,14]-loc_test[14],loc_test[14]))>0.5)\n",
    "idx_sqftFilter_compa = np.asarray(idx_sqftFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_sqftFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_sqftFilter_compa,0)\n",
    "\n",
    "idx_flipFilter_compa = np.where(feature1_compa[:,13] != loc_test[13])\n",
    "idx_flipFilter_compa = np.asarray(idx_flipFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_flipFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_flipFilter_compa,0)\n",
    "\n",
    "idx_tdFilter_compa = np.where(feature1_compa[:,4] != loc_test[4])\n",
    "idx_tdFilter_compa = np.asarray(idx_tdFilter_compa)\n",
    "feature1_compa=np.delete(feature1_compa,idx_tdFilter_compa,0)\n",
    "Y_train_compa=np.delete(Y_train_compa,idx_tdFilter_compa,0)\n",
    "\n",
    "dist_compa=[]\n",
    "for i in range(0,feature1_compa.shape[0]):\n",
    "    dist_compa.append(vincenty(loc_test[5:7], feature1_compa[i,5:7]).miles)\n",
    "    \n",
    "dist_compa=np.asarray(dist_compa)\n",
    "dist_compa_sorted = np.sort(dist_compa)\n",
    "\n",
    "top_compa= 30\n",
    "dist_select_compa= dist_compa_sorted[0:top_compa]\n",
    "loc_match_id_compa=[np.where(dist_compa == dist_select_compa[x])[0] for x in range(0,top_compa)]\n",
    "loc_match_id_compa = np.concatenate(loc_match_id_compa).ravel()\n",
    "loc_match_id_compa = np.unique(loc_match_id_compa)\n",
    "dist_selected_compa = dist_compa_sorted[0:top_compa]\n",
    "dist_feature_compa = dist_compa[loc_match_id_compa]\n",
    "Y_train_compa = Y_train_compa[loc_match_id_compa]\n",
    "feature_test_compa =np.append(loc_test, 0)\n",
    "\n",
    "feature1_compa = feature1_compa[loc_match_id_compa,:]\n",
    "feature_compa=np.vstack((feature1_compa.T, dist_feature_compa.T))\n",
    "feature_compa=feature_compa.T\n",
    "\n",
    "feature_compa = np.vstack(feature_compa[:, 3:]).astype(np.float)\n",
    "feature_test_compa = np.vstack(feature_test_compa[3:]).astype(np.float)\n",
    "\n",
    "scaler_compa = preprocessing.StandardScaler().fit(feature_compa)\n",
    "X_scaled_compa = scaler_compa.transform(feature_compa)  \n",
    "X_scaled_normed_compa = preprocessing.normalize(X_scaled_compa, norm='l2')\n",
    "\n",
    "X_train_compa = X_scaled_normed_compa\n",
    "\n",
    "feature_noNeed=[2,3,7,8,9,14,15,16]\n",
    "X_train_compa = np.delete(X_train_compa,feature_noNeed,1)\n",
    "X_test_scaled_compa = scaler_compa.transform(feature_test_compa.reshape(1, -1))  \n",
    "X_test_scaled_normed_compa= preprocessing.normalize(X_test_scaled_compa.reshape(1, -1) , norm='l2')\n",
    "X_test_compa = X_test_scaled_normed_compa\n",
    "X_test_compa = np.delete(X_test_compa,feature_noNeed,1)\n",
    "print str(X_train_compa.shape[0]) + (' houses are used for comparable house search')\n",
    "\n",
    "import math\n",
    "from itertools import izip\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return sum(map(lambda x: x[0] * x[1], izip(v1, v2)))\n",
    "\n",
    "def cosine_measure(v1, v2):\n",
    "    prod = dot_product(v1, v2)\n",
    "    len1 = math.sqrt(dot_product(v1, v1))\n",
    "    len2 = math.sqrt(dot_product(v2, v2))\n",
    "    return prod / (len1 * len2)\n",
    "\n",
    "similarity = []\n",
    "for i in range(0,X_train_compa.shape[0]):\n",
    "    similarity.append(cosine_measure(X_test_compa.reshape((-1,1)), X_train_compa[i,:].reshape((-1,1))) )\n",
    "\n",
    "    \n",
    "    \n",
    "# find the top 10 similar houses based on cosine similarity \n",
    "from astropy.table import Table, Column\n",
    "top_simi = 10\n",
    "\n",
    "simi_sorted = sorted(similarity, reverse=True)\n",
    "simi_select = simi_sorted[0:top_simi]\n",
    "simi_match_id =[np.where(similarity == simi_select[x])[0] for x in range(0,top_simi)]\n",
    "simi_match_id = np.concatenate(simi_match_id).ravel()\n",
    "simi_match_id = np.unique(simi_match_id)\n",
    "\n",
    "numBath = feature_compa[simi_match_id, 0].astype(np.uint8)\n",
    "numBed = feature_compa[simi_match_id, 4].astype(np.uint8) \n",
    "year_built = 2016-feature_compa[simi_match_id, 5].astype(np.uint16) \n",
    "lot_sqft = feature_compa[simi_match_id, 6] \n",
    "sold_year = feature_compa[simi_match_id, 8].astype(np.uint16) \n",
    "sold_month =feature_compa[simi_match_id, 9].astype(np.uint8) \n",
    "sqft=feature_compa[simi_match_id, 11] \n",
    "prop_tax_rate = feature_compa[simi_match_id, 12] \n",
    "count_perzip = feature_compa[simi_match_id, 14].astype(np.uint16) \n",
    "distance=feature_compa[simi_match_id, 17]\n",
    "zipcd = feature1_compa[simi_match_id, 1]\n",
    "propID = feature1_compa[simi_match_id, 0]\n",
    "\n",
    "\n",
    "simi = np.asarray(similarity)[simi_match_id]\n",
    "\n",
    "for i in range(0,simi_match_id.shape[0]):\n",
    "    simi[i]=float(int(simi[i]*100))/100\n",
    "    distance[i]=float(int(distance[i]*10))/10\n",
    "    \n",
    "simi=simi.reshape((-1,1))    \n",
    "sold_price = Y_train_compa[simi_match_id]\n",
    "\n",
    "t = Table([simi,propID, zipcd, numBath, numBed,year_built,lot_sqft,sold_year,sold_month,sqft,\\\n",
    "           prop_tax_rate,distance,sold_price], names=('similarity','propID','zipcd','#Bath', '#Bed',\\\n",
    "                                                            'yr._built','lot','sold_yr.',\\\n",
    "                                                            'sold_mon.','sqft','prop_taxrate','dist',\\\n",
    "                                                            'sold_price'))\n",
    "weight = np.asarray(simi/np.sum(simi)).reshape((10,))\n",
    "price_est_weighted=np.sum(np.multiply(sold_price,weight))\n",
    "price_est_median=np.median(sold_price)\n",
    "print ('the estimated price from model 2 is: ') + str(price_est_weighted)\n",
    "\n",
    "t_propID = Table([propID,year_built], names=('propID','year_built'))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('top10comp_#7219.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in t_propID]\n",
    "\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
